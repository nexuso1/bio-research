{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10405,"status":"ok","timestamp":1700568150447,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"},"user_tz":-60},"id":"wLIUpY0rj_J3","outputId":"43d44eb3-2921-426f-caf3-2cb60de59081"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":82,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36858,"status":"ok","timestamp":1700568187299,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"},"user_tz":-60},"id":"wyRdzGfXkZ2j","outputId":"b5f96b95-416e-474d-951b-ef12848d206d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: biopython in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.81)Note: you may need to restart the kernel to use updated packages.\n","\n","Requirement already satisfied: transformers in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.35.0)\n","Collecting datasets\n","  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n","Collecting evaluate\n","  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: numpy in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from biopython) (1.24.3)\n","Requirement already satisfied: filelock in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.12.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.17.3)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.10.3)\n","Requirement already satisfied: requests in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.1)\n","Collecting pyarrow>=8.0.0 (from datasets)\n","  Downloading pyarrow-14.0.1-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n","Collecting pyarrow-hotfix (from datasets)\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n","Requirement already satisfied: pandas in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.1.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n","Collecting aiohttp (from datasets)\n","  Downloading aiohttp-3.9.0-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n","Collecting responses<0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Collecting attrs>=17.3.0 (from aiohttp->datasets)\n","  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n","     ---------------------------------------- 0.0/61.2 kB ? eta -:--:--\n","     ---------------------------------------- 61.2/61.2 kB 3.4 MB/s eta 0:00:00\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n","  Downloading multidict-6.0.4-cp311-cp311-win_amd64.whl (28 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n","  Downloading yarl-1.9.3-cp311-cp311-win_amd64.whl.metadata (29 kB)\n","Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n","  Downloading frozenlist-1.4.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n","Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.0.5)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.7.22)\n","INFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.0-cp311-none-win_amd64.whl.metadata (6.8 kB)\n","Collecting evaluate\n","  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n","     ---------------------------------------- 0.0/81.4 kB ? eta -:--:--\n","     ---------------------------------------- 81.4/81.4 kB 4.4 MB/s eta 0:00:00\n","  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n","     ---------------------------------------- 0.0/72.9 kB ? eta -:--:--\n","     ---------------------------------------- 72.9/72.9 kB 3.9 MB/s eta 0:00:00\n","  Downloading evaluate-0.2.2-py3-none-any.whl (69 kB)\n","     ---------------------------------------- 0.0/69.8 kB ? eta -:--:--\n","     ---------------------------------------- 69.8/69.8 kB 4.0 MB/s eta 0:00:00\n","INFO: pip is still looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n","  Downloading evaluate-0.2.1-py3-none-any.whl (69 kB)\n","     ---------------------------------------- 0.0/69.6 kB ? eta -:--:--\n","     ---------------------------------------- 69.6/69.6 kB 4.0 MB/s eta 0:00:00\n","  Downloading evaluate-0.2.0-py3-none-any.whl (69 kB)\n","     ---------------------------------------- 0.0/69.6 kB ? eta -:--:--\n","     ---------------------------------------- 69.6/69.6 kB 4.0 MB/s eta 0:00:00\n","  Downloading evaluate-0.1.2-py3-none-any.whl (53 kB)\n","     ---------------------------------------- 0.0/53.7 kB ? eta -:--:--\n","     ---------------------------------------- 53.7/53.7 kB 2.9 MB/s eta 0:00:00\n","INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n","  Downloading evaluate-0.1.1-py3-none-any.whl (68 kB)\n","     ---------------------------------------- 0.0/68.8 kB ? eta -:--:--\n","     ---------------------------------------- 68.8/68.8 kB 3.9 MB/s eta 0:00:00\n","  Downloading evaluate-0.1.0-py3-none-any.whl (68 kB)\n","     ---------------------------------------- 0.0/68.7 kB ? eta -:--:--\n","     ---------------------------------------- 68.7/68.7 kB 1.8 MB/s eta 0:00:00\n","Collecting datasets\n","  Downloading datasets-2.14.7-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: colorama in c:\\users\\samo\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: six>=1.5 in c:\\users\\samo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Downloading datasets-2.14.7-py3-none-any.whl (520 kB)\n","   ---------------------------------------- 0.0/520.4 kB ? eta -:--:--\n","   -------------------------- ------------ 358.4/520.4 kB 10.9 MB/s eta 0:00:01\n","   ---------------------------------------- 520.4/520.4 kB 8.1 MB/s eta 0:00:00\n","Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n","   ---------------------------------------- 0.0/84.1 kB ? eta -:--:--\n","   ---------------------------------------- 84.1/84.1 kB 4.6 MB/s eta 0:00:00\n","Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n","   ---------------------------------------- 115.3/115.3 kB 6.6 MB/s eta 0:00:00\n","Downloading aiohttp-3.9.0-cp311-cp311-win_amd64.whl (364 kB)\n","   ---------------------------------------- 0.0/364.3 kB ? eta -:--:--\n","   ---------------------------------------  358.4/364.3 kB 7.4 MB/s eta 0:00:01\n","   ---------------------------------------- 364.3/364.3 kB 5.7 MB/s eta 0:00:00\n","Downloading pyarrow-14.0.1-cp311-cp311-win_amd64.whl (24.6 MB)\n","   ---------------------------------------- 0.0/24.6 MB ? eta -:--:--\n","   ---------------------------------------- 0.3/24.6 MB 8.9 MB/s eta 0:00:03\n","   - -------------------------------------- 0.7/24.6 MB 8.5 MB/s eta 0:00:03\n","   - -------------------------------------- 1.1/24.6 MB 8.8 MB/s eta 0:00:03\n","   -- ------------------------------------- 1.5/24.6 MB 8.5 MB/s eta 0:00:03\n","   --- ------------------------------------ 1.8/24.6 MB 8.4 MB/s eta 0:00:03\n","   --- ------------------------------------ 2.2/24.6 MB 8.4 MB/s eta 0:00:03\n","   ---- ----------------------------------- 2.7/24.6 MB 8.5 MB/s eta 0:00:03\n","   ---- ----------------------------------- 3.0/24.6 MB 8.4 MB/s eta 0:00:03\n","   ----- ---------------------------------- 3.4/24.6 MB 8.4 MB/s eta 0:00:03\n","   ------ --------------------------------- 3.8/24.6 MB 8.3 MB/s eta 0:00:03\n","   ------ --------------------------------- 4.2/24.6 MB 8.4 MB/s eta 0:00:03\n","   ------- -------------------------------- 4.6/24.6 MB 8.4 MB/s eta 0:00:03\n","   -------- ------------------------------- 5.0/24.6 MB 8.4 MB/s eta 0:00:03\n","   -------- ------------------------------- 5.4/24.6 MB 8.4 MB/s eta 0:00:03\n","   --------- ------------------------------ 5.8/24.6 MB 8.5 MB/s eta 0:00:03\n","   ---------- ----------------------------- 6.2/24.6 MB 8.5 MB/s eta 0:00:03\n","   ---------- ----------------------------- 6.6/24.6 MB 8.5 MB/s eta 0:00:03\n","   ----------- ---------------------------- 7.0/24.6 MB 8.5 MB/s eta 0:00:03\n","   ------------ --------------------------- 7.4/24.6 MB 8.5 MB/s eta 0:00:03\n","   ------------ --------------------------- 7.8/24.6 MB 8.5 MB/s eta 0:00:02\n","   ------------- -------------------------- 8.2/24.6 MB 8.5 MB/s eta 0:00:02\n","   -------------- ------------------------- 8.6/24.6 MB 8.5 MB/s eta 0:00:02\n","   -------------- ------------------------- 9.0/24.6 MB 8.5 MB/s eta 0:00:02\n","   --------------- ------------------------ 9.4/24.6 MB 8.6 MB/s eta 0:00:02\n","   --------------- ------------------------ 9.8/24.6 MB 8.6 MB/s eta 0:00:02\n","   ---------------- ----------------------- 10.2/24.6 MB 8.6 MB/s eta 0:00:02\n","   ----------------- ---------------------- 10.6/24.6 MB 8.6 MB/s eta 0:00:02\n","   ----------------- ---------------------- 11.0/24.6 MB 8.6 MB/s eta 0:00:02\n","   ------------------ --------------------- 11.4/24.6 MB 8.5 MB/s eta 0:00:02\n","   ------------------- -------------------- 11.7/24.6 MB 8.5 MB/s eta 0:00:02\n","   ------------------- -------------------- 12.1/24.6 MB 8.5 MB/s eta 0:00:02\n","   -------------------- ------------------- 12.5/24.6 MB 8.5 MB/s eta 0:00:02\n","   --------------------- ------------------ 12.9/24.6 MB 8.5 MB/s eta 0:00:02\n","   --------------------- ------------------ 13.3/24.6 MB 8.5 MB/s eta 0:00:02\n","   ---------------------- ----------------- 13.7/24.6 MB 8.5 MB/s eta 0:00:02\n","   ---------------------- ----------------- 14.1/24.6 MB 8.5 MB/s eta 0:00:02\n","   ----------------------- ---------------- 14.4/24.6 MB 8.5 MB/s eta 0:00:02\n","   ------------------------ --------------- 14.8/24.6 MB 8.5 MB/s eta 0:00:02\n","   ------------------------ --------------- 15.2/24.6 MB 8.5 MB/s eta 0:00:02\n","   ------------------------- -------------- 15.6/24.6 MB 8.4 MB/s eta 0:00:02\n","   ------------------------- -------------- 15.9/24.6 MB 8.4 MB/s eta 0:00:02\n","   -------------------------- ------------- 16.3/24.6 MB 8.4 MB/s eta 0:00:01\n","   --------------------------- ------------ 16.7/24.6 MB 8.4 MB/s eta 0:00:01\n","   --------------------------- ------------ 17.0/24.6 MB 8.4 MB/s eta 0:00:01\n","   ---------------------------- ----------- 17.3/24.6 MB 8.4 MB/s eta 0:00:01\n","   ---------------------------- ----------- 17.7/24.6 MB 8.3 MB/s eta 0:00:01\n","   ----------------------------- ---------- 18.1/24.6 MB 8.3 MB/s eta 0:00:01\n","   ------------------------------ --------- 18.5/24.6 MB 8.3 MB/s eta 0:00:01\n","   ------------------------------ --------- 18.9/24.6 MB 8.3 MB/s eta 0:00:01\n","   ------------------------------- -------- 19.3/24.6 MB 8.3 MB/s eta 0:00:01\n","   ------------------------------- -------- 19.6/24.6 MB 8.3 MB/s eta 0:00:01\n","   -------------------------------- ------- 20.0/24.6 MB 8.3 MB/s eta 0:00:01\n","   --------------------------------- ------ 20.4/24.6 MB 8.2 MB/s eta 0:00:01\n","   --------------------------------- ------ 20.8/24.6 MB 8.2 MB/s eta 0:00:01\n","   ---------------------------------- ----- 21.2/24.6 MB 8.2 MB/s eta 0:00:01\n","   ----------------------------------- ---- 21.6/24.6 MB 8.2 MB/s eta 0:00:01\n","   ----------------------------------- ---- 22.0/24.6 MB 8.2 MB/s eta 0:00:01\n","   ------------------------------------ --- 22.4/24.6 MB 8.2 MB/s eta 0:00:01\n","   ------------------------------------- -- 22.8/24.6 MB 8.2 MB/s eta 0:00:01\n","   ------------------------------------- -- 23.2/24.6 MB 8.2 MB/s eta 0:00:01\n","   -------------------------------------- - 23.6/24.6 MB 8.3 MB/s eta 0:00:01\n","   ---------------------------------------  24.0/24.6 MB 8.3 MB/s eta 0:00:01\n","   ---------------------------------------  24.4/24.6 MB 8.4 MB/s eta 0:00:01\n","   ---------------------------------------  24.6/24.6 MB 8.3 MB/s eta 0:00:01\n","   ---------------------------------------  24.6/24.6 MB 8.3 MB/s eta 0:00:01\n","   ---------------------------------------  24.6/24.6 MB 8.3 MB/s eta 0:00:01\n","   ---------------------------------------- 24.6/24.6 MB 7.4 MB/s eta 0:00:00\n","Downloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n","   ---------------------------------------- 0.0/135.4 kB ? eta -:--:--\n","   ---------------------------------------- 135.4/135.4 kB 8.3 MB/s eta 0:00:00\n","Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Downloading xxhash-3.4.1-cp311-cp311-win_amd64.whl (29 kB)\n","Downloading frozenlist-1.4.0-cp311-cp311-win_amd64.whl (44 kB)\n","   ---------------------------------------- 0.0/44.9 kB ? eta -:--:--\n","   ---------------------------------------- 44.9/44.9 kB ? eta 0:00:00\n","Downloading yarl-1.9.3-cp311-cp311-win_amd64.whl (75 kB)\n","   ---------------------------------------- 0.0/75.9 kB ? eta -:--:--\n","   ---------------------------------------- 75.9/75.9 kB 4.1 MB/s eta 0:00:00\n","Installing collected packages: xxhash, pyarrow-hotfix, pyarrow, multidict, frozenlist, dill, attrs, yarl, responses, multiprocess, aiosignal, aiohttp, datasets, evaluate\n","Successfully installed aiohttp-3.9.0 aiosignal-1.3.1 attrs-23.1.0 datasets-2.14.7 dill-0.3.7 evaluate-0.4.1 frozenlist-1.4.0 multidict-6.0.4 multiprocess-0.70.15 pyarrow-14.0.1 pyarrow-hotfix-0.6 responses-0.18.0 xxhash-3.4.1 yarl-1.9.3\n"]}],"source":["%pip install biopython transformers datasets evaluate"]},{"cell_type":"markdown","metadata":{"id":"00dY1kwaknvd"},"source":["# EPSD Dataset processing"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"mvTgPCRvkwGt"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from Bio import SeqIO"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"4mvhBjWBkltD"},"outputs":[],"source":["import numpy as np\n","def load_fasta(path : str):\n","    seq_iterator = SeqIO.parse(open(path), 'fasta')\n","    seq_dict = {}\n","    for seq in seq_iterator:\n","        # extract sequence id\n","        try:\n","            seq_id = seq.id.split('|')[0]\n","        except IndexError:\n","            # For some reason, some sequences do not contain uniprot ids, so skip them\n","            continue\n","        seq_dict[seq_id] = str(seq.seq)\n","\n","    return seq_dict\n","\n","def load_phospho(path : str):\n","    data = pd.read_csv(path, sep='\\t')\n","    data.index = data['EPSD ID']\n","    grouped = data.groupby(data['EPSD ID'])\n","\n","    res = {}\n","    for id, group in grouped:\n","        res[id] = group['Position'].to_list()\n","\n","    return res\n","\n","def get_inputs_outputs(fasta_path, phospho_path):\n","    fasta = load_fasta(fasta_path)\n","    phospho = load_phospho(phospho_path)\n","\n","    inputs = []\n","    targets = []\n","    for key in phospho.keys():\n","        inputs.append(fasta[key])\n","        targets.append(phospho[key])\n","\n","    return inputs, targets"]},{"cell_type":"markdown","metadata":{"id":"vu85rtonlIG1"},"source":["## Create a dataset class for training"]},{"cell_type":"code","execution_count":66,"metadata":{"id":"qsU_alwzlHX1"},"outputs":[],"source":["import re\n","\n","class ProteinDataset(Dataset):\n","    def __init__(self,tokenizer, max_length,  \n","                 inputs : list = None,\n","                 targets : list = None,\n","                 fasta_path : str = None,\n","                 phospho_path : str = None,\n","                 verbose = 1\n","                 ) -> None:\n","        # Load from file if paths given\n","        if fasta_path and phospho_path:\n","            self.x, self.y = get_inputs_outputs(fasta_path, phospho_path)\n","        \n","        # Take input from given arrays\n","        else:\n","            if verbose > 0:\n","                if inputs is None:\n","                    \n","                    print('Warning: No input path given and the inputs parameter is None')\n","                \n","                if targets is None:\n","                    print('Warning: No targets given and the targets parameter is None')\n","\n","            self.x = inputs\n","            self.y = targets\n","\n","        self.verbose = verbose\n","        self.tokenizer = tokenizer\n","        self.max_len = max_length\n","\n","        self.prune_long_sequences()\n","\n","    def prune_long_sequences(self) -> None:\n","        \"\"\"\n","        Remove all sequences that are longer than self.max_len from the dataset.\n","        Updates the self.x and self.y attributes.\n","        \"\"\"\n","        keep_x = []\n","        keep_y = []\n","\n","        count = 0\n","\n","        for i in range(len(self.x)):\n","            if len(self.x[i]) > self.max_len:\n","                count += 1\n","                continue\n","\n","            keep_x.append(self.x[i])\n","            keep_y.append(self.y[i])\n","\n","        if self.verbose > 0:\n","            print(f\"Removed {count} sequences from the dataset longer than {self.max_len}.\")\n","\n","        self.x = keep_x\n","        self.y = keep_y\n","\n","    def prep_seq(self, seq):\n","        \"\"\"\n","        Prepares the given sequence for the model by subbing rare AAs for X and adding \n","        padding between AAs. Required by the base model.\n","        \"\"\"\n","        return \" \".join(list(re.sub(r\"[UZOB]\", \"X\", seq)))\n","\n","    def prep_target(self, enc, target):\n","        \"\"\"\n","        Transforms the target into an array of ones and zeros with the same length as the \n","        corresponding FASTA protein sequence. Value of one represents a phosphorylation \n","        site being present at the i-th AA in the protein sequence.\n","        \"\"\"\n","        res = torch.zeros(self.max_len)\n","        res[target] = 1\n","        res = res.roll(1)\n","        for i, idx in enumerate(enc.input_ids.flatten().int()):\n","            if idx == 0: # [PAD]\n","                break\n","\n","            if idx == 2 or idx == 3: # [CLS] or [SEP]\n","                res[i] = -100 # This label will be ignored by the loss\n","        return res\n","\n","    def __getitem__(self, index):\n","        seq = self.x[index]\n","        target =self.y[index]\n","        seq = self.prep_seq(seq)\n","        encoding = self.tokenizer(\n","            seq,\n","            add_special_tokens=True,\n","            max_length = self.max_len,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","\n","        target = self.prep_target(encoding, target)\n","        encoding['labels'] = torch.tensor(target, dtype=torch.long)\n","        return encoding\n","\n","    def __len__(self):\n","        return len(self.x)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1700568259808,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"},"user_tz":-60},"id":"FjHGHsAelxKd","outputId":"71e91b31-b408-42cc-8b45-f61525e2fbc2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cpu\n"]}],"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(\"Using {}\".format(device))"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"x6HtBfWK34ob"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Samo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["#import dependencies\n","import os.path\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n","from torch.utils.data import DataLoader\n","\n","import re\n","import numpy as np\n","import pandas as pd\n","import copy\n","\n","import transformers\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","from transformers.models.t5.modeling_t5 import T5Config, T5PreTrainedModel, T5Stack\n","from transformers.utils.model_parallel_utils import assert_device_map, get_device_map\n","from transformers import T5EncoderModel, T5Tokenizer\n","from transformers import TrainingArguments, Trainer, set_seed\n","\n","# from evaluate import load\n","# from datasets import Dataset\n","\n","from tqdm import tqdm\n","import random\n","\n","from scipy import stats\n","from sklearn.metrics import accuracy_score\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"ho0d36DW3NFc"},"source":["## LoRA implementation\n","taken from https://github.com/r-three/t-few\n","\n","(https://github.com/r-three/t-few/blob/master/src/models/lora.py, https://github.com/r-three/t-few/tree/master/configs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8u92yr7Z3MRN"},"outputs":[],"source":["# Modifies an existing transformer and introduce the LoRA layers\n","\n","class LoRAConfig:\n","    def __init__(self):\n","        self.lora_rank = 4\n","        self.lora_init_scale = 0.01\n","        self.lora_modules = \".*SelfAttention|.*EncDecAttention\"\n","        self.lora_layers = \"q|k|v|o\"\n","        self.trainable_param_names = \".*layer_norm.*|.*lora_[ab].*\"\n","        self.lora_scaling_rank = 1\n","        # lora_modules and lora_layers are speicified with regular expressions\n","        # see https://www.w3schools.com/python/python_regex.asp for reference\n","\n","class LoRALinear(nn.Module):\n","    def __init__(self, linear_layer, rank, scaling_rank, init_scale):\n","        super().__init__()\n","        self.in_features = linear_layer.in_features\n","        self.out_features = linear_layer.out_features\n","        self.rank = rank\n","        self.scaling_rank = scaling_rank\n","        self.weight = linear_layer.weight\n","        self.bias = linear_layer.bias\n","        if self.rank > 0:\n","            self.lora_a = nn.Parameter(torch.randn(rank, linear_layer.in_features) * init_scale)\n","            if init_scale < 0:\n","                self.lora_b = nn.Parameter(torch.randn(linear_layer.out_features, rank) * init_scale)\n","            else:\n","                self.lora_b = nn.Parameter(torch.zeros(linear_layer.out_features, rank))\n","        if self.scaling_rank:\n","            self.multi_lora_a = nn.Parameter(\n","                torch.ones(self.scaling_rank, linear_layer.in_features)\n","                + torch.randn(self.scaling_rank, linear_layer.in_features) * init_scale\n","            )\n","            if init_scale < 0:\n","                self.multi_lora_b = nn.Parameter(\n","                    torch.ones(linear_layer.out_features, self.scaling_rank)\n","                    + torch.randn(linear_layer.out_features, self.scaling_rank) * init_scale\n","                )\n","            else:\n","                self.multi_lora_b = nn.Parameter(torch.ones(linear_layer.out_features, self.scaling_rank))\n","\n","    def forward(self, input):\n","        if self.scaling_rank == 1 and self.rank == 0:\n","            # parsimonious implementation for ia3 and lora scaling\n","            if self.multi_lora_a.requires_grad:\n","                hidden = F.linear((input * self.multi_lora_a.flatten()), self.weight, self.bias)\n","            else:\n","                hidden = F.linear(input, self.weight, self.bias)\n","            if self.multi_lora_b.requires_grad:\n","                hidden = hidden * self.multi_lora_b.flatten()\n","            return hidden\n","        else:\n","            # general implementation for lora (adding and scaling)\n","            weight = self.weight\n","            if self.scaling_rank:\n","                weight = weight * torch.matmul(self.multi_lora_b, self.multi_lora_a) / self.scaling_rank\n","            if self.rank:\n","                weight = weight + torch.matmul(self.lora_b, self.lora_a) / self.rank\n","            return F.linear(input, weight, self.bias)\n","\n","    def extra_repr(self):\n","        return \"in_features={}, out_features={}, bias={}, rank={}, scaling_rank={}\".format(\n","            self.in_features, self.out_features, self.bias is not None, self.rank, self.scaling_rank\n","        )\n","\n","\n","def modify_with_lora(transformer, config):\n","    for m_name, module in dict(transformer.named_modules()).items():\n","        if re.fullmatch(config.lora_modules, m_name):\n","            for c_name, layer in dict(module.named_children()).items():\n","                if re.fullmatch(config.lora_layers, c_name):\n","                    assert isinstance(\n","                        layer, nn.Linear\n","                    ), f\"LoRA can only be applied to torch.nn.Linear, but {layer} is {type(layer)}.\"\n","                    setattr(\n","                        module,\n","                        c_name,\n","                        LoRALinear(layer, config.lora_rank, config.lora_scaling_rank, config.lora_init_scale),\n","                    )\n","    return transformer"]},{"cell_type":"markdown","metadata":{"id":"O3YXT5Le3YZV"},"source":["# Classification/Regression heads\n","\n","taken from https://github.com/agemagician/ProtTrans/blob/master/Fine-Tuning/PT5_LoRA_Finetuning_per_prot.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wY0YEXQM3kLM"},"outputs":[],"source":["class ClassConfig:\n","    def __init__(self, dropout=0.2, num_labels=1):\n","        self.dropout_rate = dropout\n","        self.num_labels = num_labels\n","\n","class T5EncoderClassificationHead(nn.Module):\n","    \"\"\"Head for sentence-level classification tasks.\"\"\"\n","\n","    def __init__(self, config, class_config):\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.dropout = nn.Dropout(class_config.dropout_rate)\n","        self.out_proj = nn.Linear(config.hidden_size, class_config.num_labels)\n","\n","    def forward(self, hidden_states):\n","\n","        hidden_states =  torch.mean(hidden_states,dim=1)  # avg embedding\n","\n","        hidden_states = self.dropout(hidden_states)\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = torch.tanh(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","        hidden_states = self.out_proj(hidden_states)\n","        return hidden_states\n","\n","class T5EncoderForSimpleSequenceClassification(T5PreTrainedModel):\n","\n","    def __init__(self, config: T5Config, class_config):\n","        super().__init__(config)\n","        self.num_labels = class_config.num_labels\n","        self.config = config\n","\n","        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n","\n","        encoder_config = copy.deepcopy(config)\n","        encoder_config.use_cache = False\n","        encoder_config.is_encoder_decoder = False\n","        self.encoder = T5Stack(encoder_config, self.shared)\n","\n","        self.dropout = nn.Dropout(class_config.dropout_rate)\n","        self.classifier = T5EncoderClassificationHead(config, class_config)\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","        # Model parallel\n","        self.model_parallel = False\n","        self.device_map = None\n","\n","    def parallelize(self, device_map=None):\n","        self.device_map = (\n","            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n","            if device_map is None\n","            else device_map\n","        )\n","        assert_device_map(self.device_map, len(self.encoder.block))\n","        self.encoder.parallelize(self.device_map)\n","        self.classifier = self.classifier.to(self.encoder.first_device)\n","        self.model_parallel = True\n","\n","    def deparallelize(self):\n","        self.encoder.deparallelize()\n","        self.encoder = self.encoder.to(\"cpu\")\n","        self.model_parallel = False\n","        self.device_map = None\n","        torch.cuda.empty_cache()\n","\n","    def get_input_embeddings(self):\n","        return self.shared\n","\n","    def set_input_embeddings(self, new_embeddings):\n","        self.shared = new_embeddings\n","        self.encoder.set_input_embeddings(new_embeddings)\n","\n","    def get_encoder(self):\n","        return self.encoder\n","\n","    def _prune_heads(self, heads_to_prune):\n","        \"\"\"\n","        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n","        class PreTrainedModel\n","        \"\"\"\n","        for layer, heads in heads_to_prune.items():\n","            self.encoder.layer[layer].attention.prune_heads(heads)\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.encoder(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            inputs_embeds=inputs_embeds,\n","            head_mask=head_mask,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        hidden_states = outputs[0]\n","        logits = self.classifier(hidden_states)\n","\n","        loss = None\n","        if labels is not None:\n","            if self.config.problem_type is None:\n","                if self.num_labels == 1:\n","                    self.config.problem_type = \"regression\"\n","                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n","                    self.config.problem_type = \"single_label_classification\"\n","                else:\n","                    self.config.problem_type = \"multi_label_classification\"\n","\n","            if self.config.problem_type == \"regression\":\n","                loss_fct = MSELoss()\n","                if self.num_labels == 1:\n","                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n","                else:\n","                    loss = loss_fct(logits, labels)\n","            elif self.config.problem_type == \"single_label_classification\":\n","                loss_fct = CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            elif self.config.problem_type == \"multi_label_classification\":\n","                loss_fct = BCEWithLogitsLoss()\n","                loss = loss_fct(logits, labels)\n","        if not return_dict:\n","            output = (logits,) + outputs[1:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return SequenceClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )"]},{"cell_type":"markdown","metadata":{"id":"B9tCzXhX4awZ"},"source":["# Modified ProtT5 model\n","\n","taken from https://github.com/agemagician/ProtTrans/blob/master/Fine-Tuning/PT5_LoRA_Finetuning_per_prot.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V55DbSt64ZTN"},"outputs":[],"source":["def PT5_classification_model(num_labels):\n","    # Load PT5 and tokenizer\n","    model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n","    tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n","\n","    # Create new Classifier model with PT5 dimensions\n","    class_config=ClassConfig(num_labels=num_labels)\n","    class_model=T5EncoderForSimpleSequenceClassification(model.config,class_config)\n","\n","    # Set encoder and embedding weights to checkpoint weights\n","    class_model.shared=model.shared\n","    class_model.encoder=model.encoder\n","\n","    # Delete the checkpoint model\n","    model=class_model\n","    del class_model\n","\n","    # Print number of trainable parameters\n","    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    params = sum([np.prod(p.size()) for p in model_parameters])\n","    print(\"ProtT5_Classfier\\nTrainable Parameter: \"+ str(params))\n","\n","    # Add model modification lora\n","    config = LoRAConfig()\n","\n","    # Add LoRA layers\n","    model = modify_with_lora(model, config)\n","\n","    # Freeze Embeddings and Encoder (except LoRA)\n","    for (param_name, param) in model.shared.named_parameters():\n","                param.requires_grad = False\n","    for (param_name, param) in model.encoder.named_parameters():\n","                param.requires_grad = False\n","\n","    for (param_name, param) in model.named_parameters():\n","            if re.fullmatch(config.trainable_param_names, param_name):\n","                param.requires_grad = True\n","\n","    # Print trainable Parameter\n","    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    params = sum([np.prod(p.size()) for p in model_parameters])\n","    print(\"ProtT5_LoRA_Classfier\\nTrainable Parameter: \"+ str(params) + \"\\n\")\n","\n","    return model, tokenizer"]},{"cell_type":"markdown","metadata":{"id":"eq_gQ-cn4z5g"},"source":["# DeepSpeed config\n","\n","taken from https://github.com/agemagician/ProtTrans/blob/master/Fine-Tuning/PT5_LoRA_Finetuning_per_prot.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrneiMti42Sj"},"outputs":[],"source":["# Deepspeed config for optimizer CPU offload\n","\n","ds_config = {\n","    \"fp16\": {\n","        \"enabled\": \"auto\",\n","        \"loss_scale\": 0,\n","        \"loss_scale_window\": 1000,\n","        \"initial_scale_power\": 16,\n","        \"hysteresis\": 2,\n","        \"min_loss_scale\": 1\n","    },\n","\n","    \"optimizer\": {\n","        \"type\": \"AdamW\",\n","        \"params\": {\n","            \"lr\": \"auto\",\n","            \"betas\": \"auto\",\n","            \"eps\": \"auto\",\n","            \"weight_decay\": \"auto\"\n","        }\n","    },\n","\n","    \"scheduler\": {\n","        \"type\": \"WarmupLR\",\n","        \"params\": {\n","            \"warmup_min_lr\": \"auto\",\n","            \"warmup_max_lr\": \"auto\",\n","            \"warmup_num_steps\": \"auto\"\n","        }\n","    },\n","\n","    \"zero_optimization\": {\n","        \"stage\": 2,\n","        \"offload_optimizer\": {\n","            \"device\": \"cpu\",\n","            \"pin_memory\": True\n","        },\n","        \"allgather_partitions\": True,\n","        \"allgather_bucket_size\": 2e8,\n","        \"overlap_comm\": True,\n","        \"reduce_scatter\": True,\n","        \"reduce_bucket_size\": 2e8,\n","        \"contiguous_gradients\": True\n","    },\n","\n","    \"gradient_accumulation_steps\": \"auto\",\n","    \"gradient_clipping\": \"auto\",\n","    \"steps_per_print\": 2000,\n","    \"train_batch_size\": \"auto\",\n","    \"train_micro_batch_size_per_gpu\": \"auto\",\n","    \"wall_clock_breakdown\": False\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HGShfCNl6oMF"},"outputs":[],"source":["def preprocess_sequences(seqs, df=True):\n","    # this will replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids (PT5 needs this)\n","    if df:\n","      return df.str.replace('|'.join([\"O\",\"B\",\"U\",\"Z\"]),\"X\",regex=True)\n","    else:\n","      return [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in seqs]"]},{"cell_type":"markdown","metadata":{"id":"phuS7xmr6pX9"},"source":["## Train functions\n","taken from https://github.com/agemagician/ProtTrans/blob/master/Fine-Tuning/PT5_LoRA_Finetuning_per_prot.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3h0mXT7P5Yv2"},"outputs":[],"source":["# Set random seeds for reproducibility of your trainings run\n","def set_seeds(s):\n","    torch.manual_seed(s)\n","    np.random.seed(s)\n","    random.seed(s)\n","    set_seed(s)\n","\n","# Main training fuction\n","def train_per_protein(\n","        train_df,         #training data\n","        valid_df,         #validation data\n","        num_labels= 1,    #1 for regression, >1 for classification\n","\n","        # effective training batch size is batch * accum\n","        # we recommend an effective batch size of 8\n","        batch= 4,         #for training\n","        accum= 2,         #gradient accumulation\n","\n","        val_batch = 16,   #batch size for evaluation\n","        epochs= 10,       #training epochs\n","        lr= 3e-4,         #recommended learning rate\n","        seed= 42,         #random seed\n","        deepspeed= True,  #if gpu is large enough disable deepspeed for training speedup\n","        gpu= 1 ):         #gpu selection (1 for first gpu)\n","\n","    # Set gpu device\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(gpu-1)\n","\n","    # Set all random seeds\n","    set_seeds(seed)\n","\n","    # load model\n","    model, tokenizer = PT5_classification_model(num_labels=num_labels)\n","\n","    # Preprocess inputs\n","    # Replace uncommon AAs with \"X\"\n","    train_df[\"sequence\"]=preprocess_sequences(train_df['sequence'])\n","    valid_df[\"sequence\"]=valid_df[\"sequence\"].str.replace('|'.join([\"O\",\"B\",\"U\",\"Z\"]),\"X\",regex=True)\n","    # Add spaces between each amino acid for PT5 to correctly use them\n","    train_df['sequence']=train_df.apply(lambda row : \" \".join(row[\"sequence\"]), axis = 1)\n","    valid_df['sequence']=valid_df.apply(lambda row : \" \".join(row[\"sequence\"]), axis = 1)\n","\n","    # Create Datasets\n","    train_set=create_dataset(tokenizer,list(train_df['sequence']),list(train_df['label']))\n","    valid_set=create_dataset(tokenizer,list(valid_df['sequence']),list(valid_df['label']))\n","\n","    # Huggingface Trainer arguments\n","    args = TrainingArguments(\n","        \"./\",\n","        evaluation_strategy = \"epoch\",\n","        logging_strategy = \"epoch\",\n","        save_strategy = \"no\",\n","        learning_rate=lr,\n","        per_device_train_batch_size=batch,\n","        per_device_eval_batch_size=val_batch,\n","        gradient_accumulation_steps=accum,\n","        num_train_epochs=epochs,\n","        seed = seed,\n","        deepspeed= ds_config if deepspeed else None,\n","    )\n","\n","    # Metric definition for validation data\n","    def compute_metrics(eval_pred):\n","        if num_labels>1:  # for classification\n","            metric = load(\"accuracy\")\n","            predictions, labels = eval_pred\n","            predictions = np.argmax(predictions, axis=1)\n","        else:  # for regression\n","            metric = load(\"spearmanr\")\n","            predictions, labels = eval_pred\n","\n","        return metric.compute(predictions=predictions, references=labels)\n","\n","    # Trainer\n","    trainer = Trainer(\n","        model,\n","        args,\n","        train_dataset=train_set,\n","        eval_dataset=valid_set,\n","        tokenizer=tokenizer,\n","        compute_metrics=compute_metrics\n","    )\n","\n","    # Train model\n","    trainer.train()\n","\n","    return tokenizer, model, trainer.state.log_history"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12683,"status":"ok","timestamp":1699912561260,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"},"user_tz":-60},"id":"RJM8rFvglHUw","outputId":"54409fde-38b9-495e-f66b-2f98202b5784"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Samo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import BertForTokenClassification, BertTokenizer\n","\n","pbert = BertForTokenClassification.from_pretrained(\"Rostlab/prot_bert\")\n","pbert.config.num_labels = 2\n","tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1433,"status":"ok","timestamp":1699913407233,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"},"user_tz":-60},"id":"VwgyETE6toxa","outputId":"1a3bc9fb-fc82-4428-b80a-8a2fe449ee47"},"outputs":[{"data":{"text/plain":["{'input_ids': tensor([[ 6,  9, 15, 23, 28,  6, 29]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["example = ['A E T C Z A O']\n","enc = tokenizer(example, return_tensors='pt', add_special_tokens=False)\n","enc"]},{"cell_type":"markdown","metadata":{"id":"lr36GX3t4-fK"},"source":["# TODO: Custom protein embedding"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":249,"status":"ok","timestamp":1700572271818,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"},"user_tz":-60},"id":"z4I9yJLg9Bud"},"outputs":[],"source":["from transformers import BertModel, BertTokenizer"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":232,"status":"ok","timestamp":1700572273358,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"},"user_tz":-60},"id":"b6ekupXq9Dt9"},"outputs":[],"source":["def get_bert_model():\n","    pbert = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n","    pbert.config.num_labels = 2\n","    tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n","\n","    return pbert, tokenizer"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":25190,"status":"ok","timestamp":1700572299462,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"},"user_tz":-60},"id":"2Ridi1FT9hdD"},"outputs":[],"source":["pbert, tokenizer = get_bert_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0-KlEq8CrBLz"},"outputs":[],"source":["next(pbert.parameters()).name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dEJmYV0N9q5Z"},"outputs":[],"source":["for p in pbert.parameters():\n","  if p.name and (p.name == 'classifier.weight' or p.name == 'classifier.bias'):\n","    continue\n","\n","  p.requires_grad = False"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"l6F7-hnJtASv"},"outputs":[{"ename":"NameError","evalue":"name 'nn' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Samo\\Repos\\bio-research\\colab_model.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Samo/Repos/bio-research/colab_model.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mProteinEmbed\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Samo/Repos/bio-research/colab_model.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, base_model : nn\u001b[39m.\u001b[39mModule, dropout \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m, n_labels \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m, transfer_learning\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Samo/Repos/bio-research/colab_model.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39msuper\u001b[39m(ProteinEmbed, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n","\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"]}],"source":["class ProteinEmbed(nn.Module):\n","    def __init__(self, base_model : nn.Module, dropout = 0.2, n_labels = 2, transfer_learning=True) -> None:\n","        super(ProteinEmbed, self).__init__()\n","        self.base = base_model\n","        self.n_labels = n_labels\n","        self.dropout = nn.Dropout(dropout)\n","        self.classifier = nn.Linear(self.base.config.hidden_size, self.n_labels)\n","        self.activation = F.softmax\n","        self.init_weights()\n","\n","        if transfer_learning:\n","            self.freeze_base()\n","\n","    def init_weights(self):\n","        torch.nn.init.normal_(self.classifier.weight)\n","        torch.nn.init.zeros_(self.classifier.bias)\n","        \n","    def freeze_base(self):\n","        for p in self.base.parameters():\n","          p.requires_grad = False\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","    ):\n","        out = self.base(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","        )\n","\n","        sequence_output = out.hidden_states[-1]\n","        sequence_output = self.dropout(sequence_output)\n","        logits = self.classifier(sequence_output)\n","        outputs = self.activation(logits, 1)\n","\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss()\n","            # Only keep active parts of the loss\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                active_logits = logits.view(-1, self.n_labels)\n","                active_labels = torch.where(\n","                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n","                )\n","                loss = loss_fct(active_logits, active_labels)\n","            else:\n","                loss = loss_fct(logits.view(-1, self.n_labels), labels.view(-1))\n","            outputs = (loss, outputs)\n","\n","        return outputs  # (loss), scores, (hidden_states), (attentions)"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["test = ProteinEmbed(pbert)"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Samo\\AppData\\Local\\Temp\\ipykernel_6700\\2073962776.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  encoding['labels'] = torch.tensor(target, dtype=torch.long)\n"]},{"data":{"text/plain":["(tensor(8.5290, grad_fn=<NllLossBackward0>),\n"," tensor([[[-25.4104, -10.4442],\n","          [-11.2124, -12.3758],\n","          [-14.3751, -18.8117],\n","          ...,\n","          [-20.8925, -14.2671],\n","          [-22.0867, -13.6937],\n","          [-20.8202, -13.7789]]], grad_fn=<LogSoftmaxBackward0>))"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["test(output_hidden_states =True, **ds[0])"]},{"cell_type":"code","execution_count":94,"metadata":{"executionInfo":{"elapsed":231,"status":"ok","timestamp":1700571604246,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"},"user_tz":-60},"id":"SlEWat-ZlkVq"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","from sklearn.model_selection import train_test_split\n","import evaluate\n","\n","def set_seeds(s):\n","    torch.manual_seed(s)\n","    np.random.seed(s)\n","    random.seed(s)\n","    set_seed(s)\n","\n","def compute_metrics(eval_pred, metric):\n","    logits, labels = eval_pred\n","    preds = F.log_softmax(logits, dim=1)\n","    return metric.compute(predictions = preds, references=labels)\n","\n","def train_model(train_ds, test_ds, model, tokenizer,\n","                lr=3e-4, epochs=1, batch=50, val_batch=100, accum=1, seed=42, deepspeed=None):\n","    t_args = TrainingArguments(output_dir='bert_output', evaluation_strategy='epoch')\n","    metric = evaluate.load('accuracy')\n","\n","    # Set all random seeds\n","    set_seeds(seed)\n","\n","    # Huggingface Trainer arguments\n","    args = TrainingArguments(\n","        \"./\",\n","        evaluation_strategy = \"epoch\",\n","        logging_strategy = \"epoch\",\n","        save_strategy = \"no\",\n","        learning_rate=lr,\n","        per_device_train_batch_size=batch,\n","        per_device_eval_batch_size=val_batch,\n","        gradient_accumulation_steps=accum,\n","        num_train_epochs=epochs,\n","        seed = seed,\n","        deepspeed= ds_config if deepspeed else None,\n","    )\n","    \n","    # Trainer\n","    trainer = Trainer(\n","        model,\n","        args,\n","        train_dataset=train_ds,\n","        eval_dataset=test_ds,\n","        tokenizer=tokenizer,\n","        compute_metrics=compute_metrics\n","    )\n","\n","    # Train model\n","    trainer.train()\n","\n","    return tokenizer, model, trainer.state.log_history"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[],"source":["def main(**kwargs):\n","    inputs, outputs = get_inputs_outputs('epsd_sequences/Total.fasta', 'epsd_sequences/Total.txt')\n","    pbert, tokenizer = get_bert_model()\n","    train_X, test_X, train_y, test_y = train_test_split(inputs, outputs, random_state=kwargs['seed'])\n","\n","    train_dataset = ProteinDataset(tokenizer=tokenizer, max_length=kwargs['max_length'], inputs=train_X, targets=train_y)\n","    test_dataset = ProteinDataset(tokenizer=tokenizer, max_length=kwargs['max_length'], inputs=test_X, targets=test_y)\n","\n","    train_model(train_ds=train_dataset, test_ds=test_dataset, model=pbert, tokenizer=tokenizer, seed=kwargs['seed'])"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Removed 5171 sequences from the dataset longer than 2047.\n"]}],"source":["inputs, outputs = get_inputs_outputs('epsd_sequences/Total.fasta', 'epsd_sequences/Total.txt')\n","ds = ProteinDataset(tokenizer=tokenizer, max_length=2047, inputs=inputs, targets=outputs)"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Samo\\AppData\\Local\\Temp\\ipykernel_6700\\2073962776.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  encoding['labels'] = torch.tensor(target, dtype=torch.long)\n"]},{"data":{"text/plain":["{'input_ids': tensor([[ 2, 21,  5,  ...,  0,  0,  0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([-100,    0,    0,  ...,    0,    0,    0])}"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["ds[0]"]},{"cell_type":"code","execution_count":126,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Removed 3854 sequences from the dataset longer than 2048.\n","Removed 1311 sequences from the dataset longer than 2048.\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/3063 [00:00<?, ?it/s]"]}],"source":["main(max_length= 2048, seed = 42)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPA0kXamUa9M8sSEAFQm82t","collapsed_sections":["ho0d36DW3NFc","O3YXT5Le3YZV","B9tCzXhX4awZ","eq_gQ-cn4z5g"],"mount_file_id":"1lrDAZqIgOuL0OkBPy-BDaQaD6jnY_v7r","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}
