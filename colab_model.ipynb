{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1lrDAZqIgOuL0OkBPy-BDaQaD6jnY_v7r","authorship_tag":"ABX9TyOLve7z0nV+/+KSo3sihFqC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wLIUpY0rj_J3","executionInfo":{"status":"ok","timestamp":1699893841361,"user_tz":-60,"elapsed":9718,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"}},"outputId":"b090b8cd-eebf-4bef-907f-a9dacf443f27"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%pip install biopython transformers datasets evaluate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wyRdzGfXkZ2j","executionInfo":{"status":"ok","timestamp":1699899043954,"user_tz":-60,"elapsed":14897,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"}},"outputId":"cb22d869-b8fc-402a-f5f0-f4c0e79e9bef"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (1.81)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.0)\n","Collecting datasets\n","  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting evaluate\n","  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.23.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n","Collecting responses<0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: dill, responses, multiprocess, datasets, evaluate\n","Successfully installed datasets-2.14.6 dill-0.3.7 evaluate-0.4.1 multiprocess-0.70.15 responses-0.18.0\n"]}]},{"cell_type":"markdown","source":["# EPSD Dataset processing"],"metadata":{"id":"00dY1kwaknvd"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from Bio import SeqIO"],"metadata":{"id":"mvTgPCRvkwGt","executionInfo":{"status":"ok","timestamp":1699912116804,"user_tz":-60,"elapsed":231,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","def load_fasta(path : str):\n","    seq_iterator = SeqIO.parse(open(path), 'fasta')\n","    seq_dict = {}\n","    for seq in seq_iterator:\n","        # extract sequence id\n","        try:\n","            seq_id = seq.id.split('|')[0]\n","        except IndexError:\n","            # For some reason, some sequences do not contain uniprot ids, so skip them\n","            continue\n","        seq_dict[seq_id] = str(seq.seq)\n","\n","    return seq_dict\n","\n","def load_phospho(path : str):\n","    data = pd.read_csv(path, sep='\\t')\n","    data.index = data['EPSD ID']\n","    grouped = data.groupby(data['EPSD ID'])\n","\n","    res = {}\n","    for id, group in grouped:\n","        res[id] = group['Position'].to_list()\n","\n","    return res\n","\n","def get_inputs_outputs(fasta_path, phospho_path):\n","    fasta = load_fasta(fasta_path)\n","    phospho = load_phospho(phospho_path)\n","\n","    inputs = []\n","    targets = []\n","    for key in phospho.keys():\n","        inputs.append(fasta[key])\n","        targets.append(phospho[key])\n","\n","    return inputs, targets"],"metadata":{"id":"4mvhBjWBkltD","executionInfo":{"status":"ok","timestamp":1699912117698,"user_tz":-60,"elapsed":4,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["## Create a dataset class for training"],"metadata":{"id":"vu85rtonlIG1"}},{"cell_type":"code","source":["class ProteinDataset(Dataset):\n","    def __init__(self, fasta_path, phospho_path) -> None:\n","        self.x, self.y = get_inputs_outputs(fasta_path, phospho_path)\n","\n","    def __getitem__(self, index):\n","        return self.x[index], self.y[index]\n","\n","    def __len__(self):\n","        return len(self.x)"],"metadata":{"id":"qsU_alwzlHX1","executionInfo":{"status":"ok","timestamp":1699912119329,"user_tz":-60,"elapsed":344,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["dataset = ProteinDataset('/content/drive/MyDrive/data/epsd_sequences/Total.fasta', '/content/drive/MyDrive/data/epsd_sequences/Total.txt')"],"metadata":{"id":"xmTmcpXqlCCX","executionInfo":{"status":"ok","timestamp":1699912140895,"user_tz":-60,"elapsed":20577,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(\"Using {}\".format(device))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FjHGHsAelxKd","executionInfo":{"status":"ok","timestamp":1699912140897,"user_tz":-60,"elapsed":11,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"}},"outputId":"3767ea84-4983-4e45-d557-8ff6863d87ee"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu\n"]}]},{"cell_type":"code","source":["#import dependencies\n","import os.path\n","os.chdir(\"/content/drive/MyDrive/Notebooks\")\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n","from torch.utils.data import DataLoader\n","\n","import re\n","import numpy as np\n","import pandas as pd\n","import copy\n","\n","import transformers, datasets\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","from transformers.models.t5.modeling_t5 import T5Config, T5PreTrainedModel, T5Stack\n","from transformers.utils.model_parallel_utils import assert_device_map, get_device_map\n","from transformers import T5EncoderModel, T5Tokenizer\n","from transformers import TrainingArguments, Trainer, set_seed\n","\n","from evaluate import load\n","from datasets import Dataset\n","\n","from tqdm import tqdm\n","import random\n","\n","from scipy import stats\n","from sklearn.metrics import accuracy_score\n","\n","import matplotlib.pyplot as plt"],"metadata":{"id":"x6HtBfWK34ob","executionInfo":{"status":"ok","timestamp":1699899050935,"user_tz":-60,"elapsed":6986,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["## LoRA implementation\n","taken from https://github.com/r-three/t-few\n","\n","(https://github.com/r-three/t-few/blob/master/src/models/lora.py, https://github.com/r-three/t-few/tree/master/configs)"],"metadata":{"id":"ho0d36DW3NFc"}},{"cell_type":"code","source":["# Modifies an existing transformer and introduce the LoRA layers\n","\n","class LoRAConfig:\n","    def __init__(self):\n","        self.lora_rank = 4\n","        self.lora_init_scale = 0.01\n","        self.lora_modules = \".*SelfAttention|.*EncDecAttention\"\n","        self.lora_layers = \"q|k|v|o\"\n","        self.trainable_param_names = \".*layer_norm.*|.*lora_[ab].*\"\n","        self.lora_scaling_rank = 1\n","        # lora_modules and lora_layers are speicified with regular expressions\n","        # see https://www.w3schools.com/python/python_regex.asp for reference\n","\n","class LoRALinear(nn.Module):\n","    def __init__(self, linear_layer, rank, scaling_rank, init_scale):\n","        super().__init__()\n","        self.in_features = linear_layer.in_features\n","        self.out_features = linear_layer.out_features\n","        self.rank = rank\n","        self.scaling_rank = scaling_rank\n","        self.weight = linear_layer.weight\n","        self.bias = linear_layer.bias\n","        if self.rank > 0:\n","            self.lora_a = nn.Parameter(torch.randn(rank, linear_layer.in_features) * init_scale)\n","            if init_scale < 0:\n","                self.lora_b = nn.Parameter(torch.randn(linear_layer.out_features, rank) * init_scale)\n","            else:\n","                self.lora_b = nn.Parameter(torch.zeros(linear_layer.out_features, rank))\n","        if self.scaling_rank:\n","            self.multi_lora_a = nn.Parameter(\n","                torch.ones(self.scaling_rank, linear_layer.in_features)\n","                + torch.randn(self.scaling_rank, linear_layer.in_features) * init_scale\n","            )\n","            if init_scale < 0:\n","                self.multi_lora_b = nn.Parameter(\n","                    torch.ones(linear_layer.out_features, self.scaling_rank)\n","                    + torch.randn(linear_layer.out_features, self.scaling_rank) * init_scale\n","                )\n","            else:\n","                self.multi_lora_b = nn.Parameter(torch.ones(linear_layer.out_features, self.scaling_rank))\n","\n","    def forward(self, input):\n","        if self.scaling_rank == 1 and self.rank == 0:\n","            # parsimonious implementation for ia3 and lora scaling\n","            if self.multi_lora_a.requires_grad:\n","                hidden = F.linear((input * self.multi_lora_a.flatten()), self.weight, self.bias)\n","            else:\n","                hidden = F.linear(input, self.weight, self.bias)\n","            if self.multi_lora_b.requires_grad:\n","                hidden = hidden * self.multi_lora_b.flatten()\n","            return hidden\n","        else:\n","            # general implementation for lora (adding and scaling)\n","            weight = self.weight\n","            if self.scaling_rank:\n","                weight = weight * torch.matmul(self.multi_lora_b, self.multi_lora_a) / self.scaling_rank\n","            if self.rank:\n","                weight = weight + torch.matmul(self.lora_b, self.lora_a) / self.rank\n","            return F.linear(input, weight, self.bias)\n","\n","    def extra_repr(self):\n","        return \"in_features={}, out_features={}, bias={}, rank={}, scaling_rank={}\".format(\n","            self.in_features, self.out_features, self.bias is not None, self.rank, self.scaling_rank\n","        )\n","\n","\n","def modify_with_lora(transformer, config):\n","    for m_name, module in dict(transformer.named_modules()).items():\n","        if re.fullmatch(config.lora_modules, m_name):\n","            for c_name, layer in dict(module.named_children()).items():\n","                if re.fullmatch(config.lora_layers, c_name):\n","                    assert isinstance(\n","                        layer, nn.Linear\n","                    ), f\"LoRA can only be applied to torch.nn.Linear, but {layer} is {type(layer)}.\"\n","                    setattr(\n","                        module,\n","                        c_name,\n","                        LoRALinear(layer, config.lora_rank, config.lora_scaling_rank, config.lora_init_scale),\n","                    )\n","    return transformer"],"metadata":{"id":"8u92yr7Z3MRN","executionInfo":{"status":"ok","timestamp":1699898817435,"user_tz":-60,"elapsed":265,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["# Classification/Regression heads\n","\n","taken from https://github.com/agemagician/ProtTrans/blob/master/Fine-Tuning/PT5_LoRA_Finetuning_per_prot.ipynb"],"metadata":{"id":"O3YXT5Le3YZV"}},{"cell_type":"code","source":["class ClassConfig:\n","    def __init__(self, dropout=0.2, num_labels=1):\n","        self.dropout_rate = dropout\n","        self.num_labels = num_labels\n","\n","class T5EncoderClassificationHead(nn.Module):\n","    \"\"\"Head for sentence-level classification tasks.\"\"\"\n","\n","    def __init__(self, config, class_config):\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.dropout = nn.Dropout(class_config.dropout_rate)\n","        self.out_proj = nn.Linear(config.hidden_size, class_config.num_labels)\n","\n","    def forward(self, hidden_states):\n","\n","        hidden_states =  torch.mean(hidden_states,dim=1)  # avg embedding\n","\n","        hidden_states = self.dropout(hidden_states)\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = torch.tanh(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","        hidden_states = self.out_proj(hidden_states)\n","        return hidden_states\n","\n","class T5EncoderForSimpleSequenceClassification(T5PreTrainedModel):\n","\n","    def __init__(self, config: T5Config, class_config):\n","        super().__init__(config)\n","        self.num_labels = class_config.num_labels\n","        self.config = config\n","\n","        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n","\n","        encoder_config = copy.deepcopy(config)\n","        encoder_config.use_cache = False\n","        encoder_config.is_encoder_decoder = False\n","        self.encoder = T5Stack(encoder_config, self.shared)\n","\n","        self.dropout = nn.Dropout(class_config.dropout_rate)\n","        self.classifier = T5EncoderClassificationHead(config, class_config)\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","        # Model parallel\n","        self.model_parallel = False\n","        self.device_map = None\n","\n","    def parallelize(self, device_map=None):\n","        self.device_map = (\n","            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n","            if device_map is None\n","            else device_map\n","        )\n","        assert_device_map(self.device_map, len(self.encoder.block))\n","        self.encoder.parallelize(self.device_map)\n","        self.classifier = self.classifier.to(self.encoder.first_device)\n","        self.model_parallel = True\n","\n","    def deparallelize(self):\n","        self.encoder.deparallelize()\n","        self.encoder = self.encoder.to(\"cpu\")\n","        self.model_parallel = False\n","        self.device_map = None\n","        torch.cuda.empty_cache()\n","\n","    def get_input_embeddings(self):\n","        return self.shared\n","\n","    def set_input_embeddings(self, new_embeddings):\n","        self.shared = new_embeddings\n","        self.encoder.set_input_embeddings(new_embeddings)\n","\n","    def get_encoder(self):\n","        return self.encoder\n","\n","    def _prune_heads(self, heads_to_prune):\n","        \"\"\"\n","        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n","        class PreTrainedModel\n","        \"\"\"\n","        for layer, heads in heads_to_prune.items():\n","            self.encoder.layer[layer].attention.prune_heads(heads)\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.encoder(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            inputs_embeds=inputs_embeds,\n","            head_mask=head_mask,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        hidden_states = outputs[0]\n","        logits = self.classifier(hidden_states)\n","\n","        loss = None\n","        if labels is not None:\n","            if self.config.problem_type is None:\n","                if self.num_labels == 1:\n","                    self.config.problem_type = \"regression\"\n","                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n","                    self.config.problem_type = \"single_label_classification\"\n","                else:\n","                    self.config.problem_type = \"multi_label_classification\"\n","\n","            if self.config.problem_type == \"regression\":\n","                loss_fct = MSELoss()\n","                if self.num_labels == 1:\n","                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n","                else:\n","                    loss = loss_fct(logits, labels)\n","            elif self.config.problem_type == \"single_label_classification\":\n","                loss_fct = CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            elif self.config.problem_type == \"multi_label_classification\":\n","                loss_fct = BCEWithLogitsLoss()\n","                loss = loss_fct(logits, labels)\n","        if not return_dict:\n","            output = (logits,) + outputs[1:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return SequenceClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )"],"metadata":{"id":"wY0YEXQM3kLM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Modified ProtT5 model\n","\n","taken from https://github.com/agemagician/ProtTrans/blob/master/Fine-Tuning/PT5_LoRA_Finetuning_per_prot.ipynb"],"metadata":{"id":"B9tCzXhX4awZ"}},{"cell_type":"code","source":["def PT5_classification_model(num_labels):\n","    # Load PT5 and tokenizer\n","    model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n","    tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n","\n","    # Create new Classifier model with PT5 dimensions\n","    class_config=ClassConfig(num_labels=num_labels)\n","    class_model=T5EncoderForSimpleSequenceClassification(model.config,class_config)\n","\n","    # Set encoder and embedding weights to checkpoint weights\n","    class_model.shared=model.shared\n","    class_model.encoder=model.encoder\n","\n","    # Delete the checkpoint model\n","    model=class_model\n","    del class_model\n","\n","    # Print number of trainable parameters\n","    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    params = sum([np.prod(p.size()) for p in model_parameters])\n","    print(\"ProtT5_Classfier\\nTrainable Parameter: \"+ str(params))\n","\n","    # Add model modification lora\n","    config = LoRAConfig()\n","\n","    # Add LoRA layers\n","    model = modify_with_lora(model, config)\n","\n","    # Freeze Embeddings and Encoder (except LoRA)\n","    for (param_name, param) in model.shared.named_parameters():\n","                param.requires_grad = False\n","    for (param_name, param) in model.encoder.named_parameters():\n","                param.requires_grad = False\n","\n","    for (param_name, param) in model.named_parameters():\n","            if re.fullmatch(config.trainable_param_names, param_name):\n","                param.requires_grad = True\n","\n","    # Print trainable Parameter\n","    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    params = sum([np.prod(p.size()) for p in model_parameters])\n","    print(\"ProtT5_LoRA_Classfier\\nTrainable Parameter: \"+ str(params) + \"\\n\")\n","\n","    return model, tokenizer"],"metadata":{"id":"V55DbSt64ZTN","executionInfo":{"status":"ok","timestamp":1699899290498,"user_tz":-60,"elapsed":376,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["# DeepSpeed config\n","\n","taken from https://github.com/agemagician/ProtTrans/blob/master/Fine-Tuning/PT5_LoRA_Finetuning_per_prot.ipynb"],"metadata":{"id":"eq_gQ-cn4z5g"}},{"cell_type":"code","source":["# Deepspeed config for optimizer CPU offload\n","\n","ds_config = {\n","    \"fp16\": {\n","        \"enabled\": \"auto\",\n","        \"loss_scale\": 0,\n","        \"loss_scale_window\": 1000,\n","        \"initial_scale_power\": 16,\n","        \"hysteresis\": 2,\n","        \"min_loss_scale\": 1\n","    },\n","\n","    \"optimizer\": {\n","        \"type\": \"AdamW\",\n","        \"params\": {\n","            \"lr\": \"auto\",\n","            \"betas\": \"auto\",\n","            \"eps\": \"auto\",\n","            \"weight_decay\": \"auto\"\n","        }\n","    },\n","\n","    \"scheduler\": {\n","        \"type\": \"WarmupLR\",\n","        \"params\": {\n","            \"warmup_min_lr\": \"auto\",\n","            \"warmup_max_lr\": \"auto\",\n","            \"warmup_num_steps\": \"auto\"\n","        }\n","    },\n","\n","    \"zero_optimization\": {\n","        \"stage\": 2,\n","        \"offload_optimizer\": {\n","            \"device\": \"cpu\",\n","            \"pin_memory\": True\n","        },\n","        \"allgather_partitions\": True,\n","        \"allgather_bucket_size\": 2e8,\n","        \"overlap_comm\": True,\n","        \"reduce_scatter\": True,\n","        \"reduce_bucket_size\": 2e8,\n","        \"contiguous_gradients\": True\n","    },\n","\n","    \"gradient_accumulation_steps\": \"auto\",\n","    \"gradient_clipping\": \"auto\",\n","    \"steps_per_print\": 2000,\n","    \"train_batch_size\": \"auto\",\n","    \"train_micro_batch_size_per_gpu\": \"auto\",\n","    \"wall_clock_breakdown\": False\n","}"],"metadata":{"id":"lrneiMti42Sj","executionInfo":{"status":"ok","timestamp":1699899231609,"user_tz":-60,"elapsed":306,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["def preprocess_sequences(seqs, df=True):\n","    # this will replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids (PT5 needs this)\n","    if df:\n","      return df.str.replace('|'.join([\"O\",\"B\",\"U\",\"Z\"]),\"X\",regex=True)\n","    else:\n","      return [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in seqs]"],"metadata":{"id":"HGShfCNl6oMF","executionInfo":{"status":"ok","timestamp":1699899728193,"user_tz":-60,"elapsed":334,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["## Train functions\n","taken from https://github.com/agemagician/ProtTrans/blob/master/Fine-Tuning/PT5_LoRA_Finetuning_per_prot.ipynb"],"metadata":{"id":"phuS7xmr6pX9"}},{"cell_type":"code","source":["# Set random seeds for reproducibility of your trainings run\n","def set_seeds(s):\n","    torch.manual_seed(s)\n","    np.random.seed(s)\n","    random.seed(s)\n","    set_seed(s)\n","\n","# Dataset creation\n","def create_dataset(tokenizer,seqs,labels):\n","    tokenized = tokenizer(seqs, max_length=2048, padding=True, truncation=True)\n","    dataset = Dataset.from_dict(tokenized)\n","    dataset = dataset.add_column(\"labels\", labels)\n","\n","    return dataset\n","\n","# Main training fuction\n","def train_per_protein(\n","        train_df,         #training data\n","        valid_df,         #validation data\n","        num_labels= 1,    #1 for regression, >1 for classification\n","\n","        # effective training batch size is batch * accum\n","        # we recommend an effective batch size of 8\n","        batch= 4,         #for training\n","        accum= 2,         #gradient accumulation\n","\n","        val_batch = 16,   #batch size for evaluation\n","        epochs= 10,       #training epochs\n","        lr= 3e-4,         #recommended learning rate\n","        seed= 42,         #random seed\n","        deepspeed= True,  #if gpu is large enough disable deepspeed for training speedup\n","        gpu= 1 ):         #gpu selection (1 for first gpu)\n","\n","    # Set gpu device\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(gpu-1)\n","\n","    # Set all random seeds\n","    set_seeds(seed)\n","\n","    # load model\n","    model, tokenizer = PT5_classification_model(num_labels=num_labels)\n","\n","    # Preprocess inputs\n","    # Replace uncommon AAs with \"X\"\n","    train_df[\"sequence\"]=preprocess_sequences(train_df['sequence'])\n","    valid_df[\"sequence\"]=valid_df[\"sequence\"].str.replace('|'.join([\"O\",\"B\",\"U\",\"Z\"]),\"X\",regex=True)\n","    # Add spaces between each amino acid for PT5 to correctly use them\n","    train_df['sequence']=train_df.apply(lambda row : \" \".join(row[\"sequence\"]), axis = 1)\n","    valid_df['sequence']=valid_df.apply(lambda row : \" \".join(row[\"sequence\"]), axis = 1)\n","\n","    # Create Datasets\n","    train_set=create_dataset(tokenizer,list(train_df['sequence']),list(train_df['label']))\n","    valid_set=create_dataset(tokenizer,list(valid_df['sequence']),list(valid_df['label']))\n","\n","    # Huggingface Trainer arguments\n","    args = TrainingArguments(\n","        \"./\",\n","        evaluation_strategy = \"epoch\",\n","        logging_strategy = \"epoch\",\n","        save_strategy = \"no\",\n","        learning_rate=lr,\n","        per_device_train_batch_size=batch,\n","        per_device_eval_batch_size=val_batch,\n","        gradient_accumulation_steps=accum,\n","        num_train_epochs=epochs,\n","        seed = seed,\n","        deepspeed= ds_config if deepspeed else None,\n","    )\n","\n","    # Metric definition for validation data\n","    def compute_metrics(eval_pred):\n","        if num_labels>1:  # for classification\n","            metric = load(\"accuracy\")\n","            predictions, labels = eval_pred\n","            predictions = np.argmax(predictions, axis=1)\n","        else:  # for regression\n","            metric = load(\"spearmanr\")\n","            predictions, labels = eval_pred\n","\n","        return metric.compute(predictions=predictions, references=labels)\n","\n","    # Trainer\n","    trainer = Trainer(\n","        model,\n","        args,\n","        train_dataset=train_set,\n","        eval_dataset=valid_set,\n","        tokenizer=tokenizer,\n","        compute_metrics=compute_metrics\n","    )\n","\n","    # Train model\n","    trainer.train()\n","\n","    return tokenizer, model, trainer.state.log_history"],"metadata":{"id":"3h0mXT7P5Yv2","executionInfo":{"status":"ok","timestamp":1699899732624,"user_tz":-60,"elapsed":301,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["from transformers import BertForTokenClassification, BertTokenizer\n","\n","pbert = BertForTokenClassification.from_pretrained(\"Rostlab/prot_bert\")\n","pbert.config.num_labels = 2\n","tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RJM8rFvglHUw","executionInfo":{"status":"ok","timestamp":1699912561260,"user_tz":-60,"elapsed":12683,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"}},"outputId":"54409fde-38b9-495e-f66b-2f98202b5784"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["example = ['A E T C Z A O']\n","enc = tokenizer(example, return_tensors='pt', add_special_tokens=False)\n","with torch.no_grad():\n","  output = pbert(**enc).logits.argmax(-1)\n","  print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VwgyETE6toxa","executionInfo":{"status":"ok","timestamp":1699913407233,"user_tz":-60,"elapsed":1433,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"}},"outputId":"1a3bc9fb-fc82-4428-b80a-8a2fe449ee47"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 1, 1, 1, 1, 1, 1]])\n"]}]},{"cell_type":"markdown","source":["# TODO: Custom protein embedding"],"metadata":{"id":"lr36GX3t4-fK"}},{"cell_type":"code","source":["class ProteinEmbed(nn.Module):\n","    def __init__(self, **kwargs) -> None:\n","        super(ProteinEmbed, self).__init__()\n","        self.encoder, self.tokenizer = get_T5_model()\n","        self.decoder = torch.nn.Transformer(**kwargs)\n","        self.freeze_encoder()\n","\n","    def freeze_encoder(self):\n","        for p in self.encoder.parameters():\n","            p.requires_grad = False\n","\n","    def forward(self, input, target):\n","        emb = self.encoder(input)\n","        decoded = self.decoder(emb, target)\n","        decoded = decoded.view(-1, self.ntoken)\n","        return F.log_softmax(decoded, dim=1), hidden"],"metadata":{"id":"l6F7-hnJtASv","executionInfo":{"status":"ok","timestamp":1699896112131,"user_tz":-60,"elapsed":270,"user":{"displayName":"Samuel Fanči","userId":"10643939452898974935"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["\n","\n","\n","def train_model(data, target):\n","    prep_data = preprocess_sequences(data)\n"],"metadata":{"id":"SlEWat-ZlkVq"},"execution_count":null,"outputs":[]}]}